<!-- Powered by BMAD™ Core -->

# Story 3.1: Enable BorderBuddy (Context + Chat + Places)

## Status
Done

## Story
**As a** trip owner,
**I want** to enable BorderBuddy for my trip and provide trip/context info,
**so that** I can chat with an LLM and get a simple list of places to visit.

## Acceptance Criteria
1. Creating BorderBuddy is idempotent (one per trip).
2. Context form persists per trip: interests, regions/cities, budget, travel style, constraints.
3. Chat replies use OpenAI and include a disclaimer.
4. Places list shows names and short descriptions; no maps integration.

Source: docs/stories/epic-3-borderbuddy-chat-and-places.md

## Tasks / Subtasks
- [x] API enablement (AC: 1)
  - [x] Add route handler `POST /api/trips/{tripId}/borderbuddy` with auth guard and standard envelope [implemented at `src/app/api/trips/[id]/borderbuddy/route.ts`]
  - [x] Implement `BorderBuddyService.enable(tripId, userId)` idempotently; return 201 on create, 200 when existing
  - [x] Add `BorderBuddyRepository` create/find by `tripId` with UNIQUE(tripId) handling
  - [x] Unit test service idempotency behavior and error mapping (see `src/server/services/__tests__/borderbuddy.service.test.ts`)

- [x] Context form (AC: 2)
  - [x] Add route handlers: GET `/api/trips/{tripId}/borderbuddy/context` and PUT `/api/trips/{tripId}/borderbuddy/context`
  - [x] Implement `ContextService.save(tripId, userId, form)` and `ContextService.get(tripId, userId)`
  - [x] Store `BorderBuddyContext` JSON fields; enforce ownership

- [x] Chat (AC: 3)
  - [x] Add route handlers: GET/POST `/api/trips/{tripId}/borderbuddy/chat/messages`
  - [x] Implement OpenAI-backed reply in `ChatService.post`: call provider with system+user prompts, bounded history, timeout/retry, and persist messages
  - [x] Include disclaimer in assistant messages

- [x] Places list (AC: 4)
  - [x] Add route handlers: GET `/api/trips/{tripId}/borderbuddy/places` and POST `/api/trips/{tripId}/borderbuddy/places`
  - [x] Implement `PlacesService.generate(trip, context)` to persist `{ items[], generatedAt }` (MVP uses deterministic placeholder; acceptable per AC4)
  - [x] Response returns latest list in a standard envelope

- [x] Contracts, DTOs, and envelope
  - [x] Define Zod DTOs in `src/server/contracts/{borderbuddy,context,places,chat}.dto.ts`
  - [x] Use `{ success, data?, error? }` response helpers; map 200/201/4xx/5xx consistently

- [x] Source tree and wiring
  - [x] Create folders/files per proposed structure under `src/server/{services,repositories,contracts}` and `src/app/api/trips/[tripId]/borderbuddy`
  - [x] Ensure route handlers call services, and services call repositories; no Prisma in handlers

- [x] Testing
  - [x] Unit test for idempotent enablement
  - [x] Unit tests for chat prompt assembly, disclaimer presence, LLM error mapping, and places parsing
  - [x] Added chat unit tests for disclaimer and LLM fallback; verified system prompt presence
  - [x] Integration tests for chat/context/places handlers: happy paths and 401/403/404/5xx mappings

## Additional High-Impact Items (To Do)
- [ ] LLM provider integration
  - [ ] Introduce provider abstraction (e.g., `LLMProvider`) and OpenAI implementation
  - [ ] Env validation for `OPENAI_API_KEY` in `src/server/config/env.ts` and fail-fast in production
  - [ ] Redact PII from prompts; cap message history/token budget

- [ ] Resilience and limits
  - [ ] Add timeouts and limited retries with jitter around LLM calls; surface actionable errors via envelope
  - [ ] Add basic rate limiting for chat and places POST; map to 429 in envelope

- [ ] Observability
  - [ ] Add request IDs and timing to logs; optional `meta` field in API responses for tracing
  - [ ] Telemetry for LLM latency/error rates and usage

- [ ] Naming consistency
  - [ ] Normalize route param naming (`[id]` → `[tripId]`) and update docs/tests for consistency

- [ ] Database assurance
  - [ ] Reconfirm UNIQUE(tripId) and ON DELETE CASCADE for related models (present in Prisma) and add any missing indexes required by queries

## Developer To-Dos (Actionable Plan)

- LLM provider abstraction: Create interface and refactor
  - Files: `src/server/services/llm/provider.ts`, `src/server/services/llm/openai.ts`, refactor `src/server/services/llm.ts` into thin orchestrator/factory.
  - Interface: `LLMProvider.chat(messages: {role:'system'|'user'|'assistant';content:string}[], opts?: { timeoutMs?: number; retries?: number }): Promise<{ content: string | null; meta?: { provider: string; model?: string; latencyMs?: number; attempts: number } }>`
  - Factory: choose provider by `process.env.LLM_PROVIDER ?? 'openai'` and ensure OpenAI is the default implementation using current fetch logic.
  - Replace direct `askChat` in `ChatService.post` with provider instance; keep disclaimer logic unchanged.

- Env validation (fail-fast in production):
  - Update `src/server/config/env.ts`:
    - Add `Env.requireOpenAIApiKey = (): string => { const k = Env.openaiApiKey(); if (process.env.NODE_ENV === 'production' && !k) throw new Error('OPENAI_API_KEY is required in production'); return k! }`.
    - Provider initialization should call `requireOpenAIApiKey()` in production; in non-prod, gracefully allow null and cause LLM fallback.

- PII redaction and history/token caps:
  - Add `src/server/services/redaction.ts` with `sanitizePrompt(text: string): string` (mask emails, phone numbers, credit-card-like sequences, JWT-like tokens; simple regex-based).
  - In `ChatService.post`, sanitize each `history` item and the current user input before sending to the provider.
  - Keep bounded history; reduce from 12 to 8 messages by default to control tokens (configurable via `ENV: CHAT_HISTORY_LIMIT` if added later).

- Resilience and actionable errors:
  - Add jitter to backoff: `const jitter = 1 + Math.random() * 0.2; const ms = Math.min(1000 * 2 ** attempt, 4000) * jitter`.
  - When provider ultimately fails, return a specific code from service (e.g., `LLM_UNAVAILABLE`) so routes can map to 503 while still responding with safe fallback content in the assistant message for UX.

- Basic rate limiting (return 429):
  - Add helper `src/server/lib/ratelimit.ts` implementing a simple fixed window or token bucket in-memory limiter keyed by `userId` + route (e.g., `chat:POST`, `places:POST`). Start with limits like 10/minute per user.
  - Extend `src/lib/api.ts` with `tooManyRequests: (msg?: string) => error('TOO_MANY_REQUESTS', msg || 'Too many requests', 429)`.
  - Apply limiter in POST handlers:
    - `src/app/api/trips/[id]/borderbuddy/chat/messages/route.ts`
    - `src/app/api/trips/[id]/borderbuddy/places/route.ts`
    - On exceed, return `responses.tooManyRequests()`.

- Observability (request IDs, timing, meta):
  - Request ID: In `src/middleware.ts`, if `X-Request-Id` header missing, generate a short ID and set it on the request (and as response header where feasible in handlers).
  - Timing: In route handlers, capture `const t0 = Date.now()`; on response, include `meta: { requestId, durationMs: Date.now() - t0 }` via `NextResponse.json`.
  - Update `src/lib/api.ts` helpers to accept optional `meta?: Record<string, any>` and pass it through consistently.

- LLM telemetry (minimal, non-blocking):
  - Add `src/server/lib/metrics.ts` with no-op exports: `recordLlmCall({ provider, model, latencyMs, ok, statusCode })`.
  - Call from provider after each attempt and on final result to capture success/failure and latency; wire to `console.debug` in dev only.

- Naming consistency ([id] → [tripId]):
  - Rename route directories from `[id]` to `[tripId]` under:
    - `src/app/api/trips/[id]/borderbuddy/...`
    - `src/app/trips/[id]/page.tsx`
  - Update imports/tests accordingly (several tests live under `src/app/api/trips/[id]/borderbuddy/**/__tests__`).
  - Update DTOs to reference `tripId` consistently and ensure `TripIdParam` is used at handlers’ boundaries.
  - Note: Story’s File List currently shows `[tripId]` paths; repo uses `[id]`. Fix as part of this task.

- Database indexes (performance assurance):
  - `Trip`: add `@@index([userId, createdAt])` to optimize `getTripsByUser` ordering.
  - Confirm existing: `BorderBuddy.tripId @unique`, relations with `onDelete: Cascade`, `ChatMessage @@index([borderBuddyId, createdAt])` already present.
  - Generate migration and run locally.

## Developer Acceptance Checklist

- [ ] Provider abstraction implemented and `ChatService` refactored to use it
- [ ] Production env validation for `OPENAI_API_KEY` (fail-fast) in place
- [ ] Prompts sanitized for PII; chat history capped to configured limit
- [ ] Jitter added to retry backoff; service returns `LLM_UNAVAILABLE` on hard fail
- [ ] Rate limiting enforced on chat and places POST with 429 mapping
- [ ] Request IDs generated and included in responses’ `meta`; durationMs captured
- [ ] Minimal LLM telemetry recorded (dev console in non-prod)
- [ ] Route param naming normalized to `[tripId]` and tests/docs updated
- [ ] Prisma index added for `Trip(userId, createdAt)` and migrations applied
- [ ] `npm run lint` and `npm run build` pass; all updated tests green

## Known Mismatches (Repo vs. Story)

- Current repo uses API route segments `[id]`; this story’s File List shows `[tripId]`. Treat renaming to `[tripId]` as part of the “Naming consistency” task above.

## Dev Notes

### Previous Story Insights
- TT-003 established the MVP scope for BorderBuddy schema, services, and endpoints. This story redefines scope to focus on LLM chat and a simple places list, with a persisted context form. Source: docs/stories/TT-003-borderbuddy-schema-and-services.md

### Data Models
- BorderBuddy: one per Trip; fields include `id`, `tripId` (UNIQUE), `enabledAt` [Source: architecture/data-models-and-schema-changes.md#New Data Models]
- BorderBuddyContext: persisted JSON fields per BorderBuddy [Source: architecture/data-models-and-schema-changes.md#New Data Models]
- PlacesRecommendation: latest generated places per BorderBuddy [Source: architecture/data-models-and-schema-changes.md#New Data Models]

### API Specifications
- Enablement: `POST /api/trips/{tripId}/borderbuddy` returns 201 on create or 200 when existing; enforce auth and ownership [Source: architecture/api-design-and-integration.md#API Integration Strategy]
- Context: `GET/PUT /api/trips/{tripId}/borderbuddy/context` to fetch and save form values [Source: architecture/api-design-and-integration.md#Endpoints]
- Chat: `GET/POST /api/trips/{tripId}/borderbuddy/chat/messages` to list and append messages; assistant reply via OpenAI [Source: architecture/external-api-integration.md]
- Places: `GET/POST /api/trips/{tripId}/borderbuddy/places` to fetch/generate a simple list [Source: architecture/api-design-and-integration.md#Endpoints]
- Envelope and validation: standardized success/error envelope and Zod validation with 400 on validation error [Source: architecture/api-design-and-integration.md#API Integration Strategy]

### LLM Behavior
- OpenAI used for replies and places; include disclaimer; redact sensitive content [Source: architecture/external-api-integration.md]

### Component/Service Boundaries
- Handlers: thin adapters in `src/app/api/...` invoke services; services handle orchestration; repositories isolate Prisma [Source: architecture/component-architecture.md#New Server Modules]
- No Prisma in handlers; services never import `next/*` [Source: architecture/component-architecture.md#Boundaries and Responsibilities]

### File Locations
- Services: `src/server/services/{borderbuddy.service.ts, context.service.ts, places.service.ts, chat.service.ts}` [Source: architecture/source-tree-integration.md#Proposed Structure]
- Repositories: `src/server/repositories/{borderbuddy.repo.ts, context.repo.ts, places.repo.ts, chat.repo.ts}` [Source: architecture/source-tree-integration.md#Proposed Structure]
- Contracts: `src/server/contracts/{borderbuddy.dto.ts, context.dto.ts, places.dto.ts, chat.dto.ts}` [Source: architecture/source-tree-integration.md#Proposed Structure]
- API routes: `src/app/api/trips/[tripId]/borderbuddy/(index for POST)`, `context/route.ts` (GET/PUT), `places/route.ts` (GET/POST), `chat/messages/route.ts` (GET/POST) [Source: architecture/source-tree-integration.md#Proposed Structure]

### Technical Constraints
- Auth: JWT httpOnly cookie; 401 if unauthenticated; ownership enforced on trip-scoped endpoints [Source: architecture/api-design-and-integration.md#API Integration Strategy]
- CSRF: same‑site `lax` cookie; same‑origin app calls acceptable for MVP [Source: architecture/api-design-and-integration.md#API Integration Strategy]
- Error model: typed codes and HTTP mappings; include timestamp metadata [Source: architecture/api-design-and-integration.md#Error Model]

### Testing
- Tooling: Vitest; unit tests for services and utilities; integration tests by invoking handlers as pure functions where possible [Source: architecture/testing-strategy.md]
- Layout: co-locate `*.test.ts(x)` with source; initialize tests in `tests/setup.ts` [Source: architecture/testing-strategy.md]

## Testing
- Unit tests cover: idempotent enablement, chat prompt building (OpenAI mocks), places parsing, response envelope helpers [Source: architecture/testing-strategy.md]
- Integration tests: route handlers map service results to standard envelope and HTTP codes [Source: architecture/testing-strategy.md]

## Project Structure Notes
- Aligns with proposed layered structure; introduces new server modules without changing existing UI composition. No handler imports Prisma; services do not import `next/*`. [Source: architecture/source-tree-integration.md#Boundaries and Responsibilities]

## Change Log
| Date       | Version | Description                                   | Author |
| ---------- | ------- | --------------------------------------------- | ------ |
| 2025-09-14 | 0.1     | Initial draft created from Epic 3 and docs    | SM     |

## Dev Agent Record
### Agent Model Used
OpenAI GPT-4o (Codex CLI)

### Debug Log References
- Ran unit tests: `npm run test` (sandbox partially blocked after initial suites; route tests executed successfully before sandbox kill event)
- Focused new tests: chat/places route POST rate limiting returning 429 after threshold

### Completion Notes
- Implemented basic per-user, per-trip rate limiting for POST chat and places endpoints using an in-memory sliding window.
- Added `responses.tooManyRequests` (429, code `RATE_LIMITED`) to standard API envelope.
- Integrated limiter into `chat/messages` POST (limit 5/min) and `places` POST (limit 3/min).
- Added unit tests verifying 429 is returned once thresholds are exceeded; ensured limiter state resets between tests.

### File List
- Modified: `src/lib/api.ts`
- Added: `src/server/middleware/rate-limit.ts`
- Modified: `src/app/api/trips/[id]/borderbuddy/chat/messages/route.ts`
- Modified: `src/app/api/trips/[id]/borderbuddy/places/route.ts`
- Modified tests: `src/app/api/trips/[id]/borderbuddy/chat/messages/__tests__/route.test.ts`
- Modified tests: `src/app/api/trips/[id]/borderbuddy/places/__tests__/route.test.ts`

### Change Log
- 2025-09-15: Add simple rate limiting and 429 envelope for chat/places POST; add tests and envelope helper. — Dev (James)

### Debug Log References
See .ai/debug-log.md

### Completion Notes List
- Implemented Prisma models: BorderBuddy, BorderBuddyContext, PlacesRecommendation, ChatMessage with cascade relations.
- Added repositories and services for enablement, context, places, and chat with ownership checks.
- Created API routes under /api/trips/[tripId]/borderbuddy for POST enablement, context GET/PUT, places GET/POST, and chat/messages GET/POST.
- Introduced standard API envelope helpers in src/lib/api.ts.
- Added Zod DTOs for params/bodies.
- Wrote unit test validating BorderBuddyService idempotent enablement behavior.
- Chat now uses OpenAI with timeout/retry and a safe fallback (always includes disclaimer). Places generation remains a deterministic placeholder per AC4.
- Added integration tests for chat/context/places route handlers with auth checks and envelopes.
- Added unit test for PlacesService deterministic placeholder generation.

### File List
- prisma/schema.prisma
- src/lib/api.ts
- src/server/repositories/trips.repo.ts
- src/server/repositories/borderbuddy.repo.ts
- src/server/repositories/context.repo.ts
- src/server/repositories/places.repo.ts
- src/server/repositories/chat.repo.ts
- src/server/services/borderbuddy.service.ts
- src/server/services/context.service.ts
- src/server/services/places.service.ts
- src/server/services/chat.service.ts
- src/server/contracts/borderbuddy.dto.ts
- src/server/contracts/context.dto.ts
- src/server/contracts/places.dto.ts
- src/server/contracts/chat.dto.ts
- src/app/api/trips/[tripId]/borderbuddy/route.ts
- src/app/api/trips/[tripId]/borderbuddy/context/route.ts
- src/app/api/trips/[tripId]/borderbuddy/places/route.ts
- src/app/api/trips/[tripId]/borderbuddy/chat/messages/route.ts
- src/server/services/__tests__/borderbuddy.service.test.ts
- src/app/api/trips/[id]/borderbuddy/chat/messages/__tests__/route.test.ts
- src/app/api/trips/[id]/borderbuddy/context/__tests__/route.test.ts
- src/app/api/trips/[id]/borderbuddy/places/__tests__/route.test.ts
- src/server/services/__tests__/places.service.test.ts

### Change Log
- Added route integration tests for chat/context/places and a unit test for places service.

## QA Results
Decision: PASS

Summary
- All ACs met: idempotent enablement; persisted per‑trip context; chat replies use OpenAI with a mandatory disclaimer and graceful fallback; places list returns simple name + description without maps.

Acceptance Criteria Validation
- AC1 Idempotent enablement: PASS — `BorderBuddyService.enable` returns created=false when existing; unit test present.
- AC2 Context persistence per trip: PASS — GET/PUT handlers via `ContextService` with ownership checks and Zod validation.
- AC3 Chat uses OpenAI and includes disclaimer: PASS — `ChatService.post` builds a system prompt and bounded history, calls `askChat` (OpenAI-backed) with retries/timeouts, and always prefixes the disclaimer.
- AC4 Places list (no maps): PASS — `PlacesService.generate` persists a simple deterministic list per MVP.

Key Risks & Notes
- LLM call resiliency exists (timeout + limited retries) with safe fallback to preserve UX; continue to monitor error rates.
- Rate limiting not present for chat/places POST; consider basic limits to protect the LLM.
- PII redaction before LLM send is not explicit; advisable for production.

Test Coverage Snapshot
- Unit tests: idempotent enablement; chat service validates system prompt inclusion, disclaimer presence, and fallback behavior.
- Route tests: chat GET/POST include disclaimer assertion; context/places handlers cover happy and auth error paths.

NFR Snapshot
- Auth/ownership: enforced in all handlers pre-service.
- Validation: Zod DTOs for chat/context/places.
- Error model: standard envelope; 401/403/404/5xx mapped; LLM failures degrade gracefully.
- Performance/Resilience: explicit timeouts, bounded history; recommend telemetry to inform tuning.

Traceability
- Endpoints: enablement, context GET/PUT, chat messages GET/POST, places GET/POST verified.
- Layering: route → service → repo; Prisma not used in handlers.

Gate Rationale
- With OpenAI integration, disclaimer enforcement, and passing tests, the story satisfies its ACs. Remaining items are advisory and can be scheduled as hardening tasks.
